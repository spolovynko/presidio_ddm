{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "from itertools import zip_longest\n",
    "import re\n",
    "\n",
    "file_path = r\"C:\\Users\\spolo\\OneDrive\\Documents\\DOC\\Work\\TCS\\Code\\dynamic_data_masking\\presidio_ddm\\deny_list_dataset\\deny_list_tokens_ddm.xlsx\"\n",
    "new_file_path = r\"C:\\Users\\spolo\\OneDrive\\Documents\\DOC\\Work\\TCS\\Code\\dynamic_data_masking\\presidio_ddm\\deny_list_dataset\\deny_list_tokens_c4_v1.xlsx\"\n",
    "\n",
    "df = pd.read_excel(file_path, sheet_name=\"CRIME\")\n",
    "\n",
    "nlp = spacy.load(\"fr_core_news_lg\")\n",
    "french_stopwords = nlp.Defaults.stop_words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from itertools import zip_longest\n",
    "\n",
    "def augmentation(df, french_stopwords):\n",
    "    new_df_columns = {}\n",
    "\n",
    "    for col in df.columns:\n",
    "        new_column = list(df[col].dropna())  # Remove NaN values\n",
    "\n",
    "        # Split words by space\n",
    "        new_column_split = [conv.split() for conv in new_column]\n",
    "        new_column_split_joined = [item for lst in new_column_split for item in lst]\n",
    "        new_column.extend(new_column_split_joined)\n",
    "\n",
    "        # Remove French stopwords\n",
    "        new_column = [word for word in new_column if word.lower() not in french_stopwords]\n",
    "\n",
    "        # Handle hyphenated words\n",
    "        split = [sub.split(\"-\") for sub in new_column if '-' in sub]\n",
    "        new_subcat = [item for lst in split for item in lst]\n",
    "        new_column.extend(new_subcat)\n",
    "\n",
    "        # Lowercase transformation\n",
    "        new_column_lower = [word.lower() for word in new_column]\n",
    "        new_column.extend(new_column_lower)\n",
    "\n",
    "        # Remove stopwords again after modifications\n",
    "        new_column = [word for word in new_column if word.lower() not in french_stopwords]\n",
    "\n",
    "        # Convert to unique sorted list\n",
    "        new_column = sorted(set(new_column))\n",
    "\n",
    "        # Store results in dictionary (column_name: column_values)\n",
    "        new_df_columns[col] = new_column\n",
    "\n",
    "    # Make all columns the same length using zip_longest\n",
    "    max_length = max(len(v) for v in new_df_columns.values())\n",
    "    for col in new_df_columns:\n",
    "        new_df_columns[col] += [None] * (max_length - len(new_df_columns[col]))  # Padding with None\n",
    "\n",
    "    # Convert dictionary to DataFrame\n",
    "    new_df = pd.DataFrame(new_df_columns)\n",
    "\n",
    "    return new_df\n",
    "\n",
    "\n",
    "new_df = augmentation(df, french_stopwords)\n",
    "\n",
    "with pd.ExcelWriter(new_file_path, mode='a', if_sheet_exists='new', engine='openpyxl') as writer:\n",
    "    new_df.to_excel(writer, sheet_name='CRIME', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CONVICTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ARREST_RECORD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conviction = list(crime[\"ARREST_RECORD\"].dropna())\n",
    "conviction_split  = [conv.split() for conv in conviction]\n",
    "conviction_split_joined = [item for lst in conviction_split for item  in lst]\n",
    "conviction.extend(conviction_split_joined)\n",
    "\n",
    "conviction = [word for word in  conviction if word not in french_stopwords]\n",
    "split = [sub.split(\"-\") for sub in conviction if '-' in sub]\n",
    "new_subcat = [item for lst in split for item in lst]\n",
    "conviction.extend(new_subcat)\n",
    "\n",
    "conviction_lower = [word.lower() for word in conviction]\n",
    "conviction.extend(conviction_lower)\n",
    "\n",
    "conviction = [word for word in  conviction if word not in french_stopwords]\n",
    "arrest_record = set(conviction)\n",
    "sorted(arrest_record)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CONVICTION_DETAILS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conviction = list(crime[\"CONVICTION_DETAILS\"].dropna())\n",
    "conviction_split  = [conv.split() for conv in conviction]\n",
    "conviction_split_joined = [item for lst in conviction_split for item  in lst]\n",
    "conviction.extend(conviction_split_joined)\n",
    "\n",
    "conviction = [word for word in  conviction if word not in french_stopwords]\n",
    "split = [sub.split(\"-\") for sub in conviction if '-' in sub]\n",
    "new_subcat = [item for lst in split for item in lst]\n",
    "conviction.extend(new_subcat)\n",
    "\n",
    "conviction_lower = [word.lower() for word in conviction]\n",
    "conviction.extend(conviction_lower)\n",
    "\n",
    "conviction = [word for word in  conviction if word not in french_stopwords]\n",
    "conviction_details = set(conviction)\n",
    "sorted(conviction_details)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PAROLE_STATUS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conviction = list(crime[\"PAROLE_STATUS\"].dropna())\n",
    "conviction_split  = [conv.split() for conv in conviction]\n",
    "conviction_split_joined = [item for lst in conviction_split for item  in lst]\n",
    "conviction.extend(conviction_split_joined)\n",
    "\n",
    "conviction = [word for word in  conviction if word not in french_stopwords]\n",
    "split = [sub.split(\"-\") for sub in conviction if '-' in sub]\n",
    "new_subcat = [item for lst in split for item in lst]\n",
    "conviction.extend(new_subcat)\n",
    "\n",
    "conviction_lower = [word.lower() for word in conviction]\n",
    "conviction.extend(conviction_lower)\n",
    "\n",
    "conviction = [word for word in  conviction if word not in french_stopwords]\n",
    "parole_status = set(conviction)\n",
    "sorted(parole_status)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WARRANT_DETAILS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conviction = list(crime[\"PAROLE_STATUS\"].dropna())\n",
    "conviction_split  = [conv.split() for conv in conviction]\n",
    "conviction_split_joined = [item for lst in conviction_split for item  in lst]\n",
    "conviction.extend(conviction_split_joined)\n",
    "\n",
    "conviction = [word for word in  conviction if word not in french_stopwords]\n",
    "split = [sub.split(\"-\") for sub in conviction if '-' in sub]\n",
    "new_subcat = [item for lst in split for item in lst]\n",
    "conviction.extend(new_subcat)\n",
    "\n",
    "conviction_lower = [word.lower() for word in conviction]\n",
    "conviction.extend(conviction_lower)\n",
    "\n",
    "conviction = [word for word in  conviction if word not in french_stopwords]\n",
    "warrant_details = set(conviction)\n",
    "sorted(warrant_details)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FINANCIAL_CRIME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conviction = list(crime[\"PAROLE_STATUS\"].dropna())\n",
    "conviction_split  = [conv.split() for conv in conviction]\n",
    "conviction_split_joined = [item for lst in conviction_split for item  in lst]\n",
    "conviction.extend(conviction_split_joined)\n",
    "\n",
    "conviction = [word for word in  conviction if word not in french_stopwords]\n",
    "split = [sub.split(\"-\") for sub in conviction if '-' in sub]\n",
    "new_subcat = [item for lst in split for item in lst]\n",
    "conviction.extend(new_subcat)\n",
    "\n",
    "conviction_lower = [word.lower() for word in conviction]\n",
    "conviction.extend(conviction_lower)\n",
    "\n",
    "conviction = [word for word in  conviction if word not in french_stopwords]\n",
    "financial_crime = set(conviction)\n",
    "sorted(financial_crime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = augmentation(crime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "10 columns passed, passed data had 1 columns",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\pandas\\core\\internals\\construction.py:939\u001b[0m, in \u001b[0;36m_finalize_columns_and_data\u001b[1;34m(content, columns, dtype)\u001b[0m\n\u001b[0;32m    938\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 939\u001b[0m     columns \u001b[38;5;241m=\u001b[39m \u001b[43m_validate_or_indexify_columns\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcontents\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    940\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m    941\u001b[0m     \u001b[38;5;66;03m# GH#26429 do not raise user-facing AssertionError\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\pandas\\core\\internals\\construction.py:986\u001b[0m, in \u001b[0;36m_validate_or_indexify_columns\u001b[1;34m(content, columns)\u001b[0m\n\u001b[0;32m    984\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_mi_list \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(columns) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(content):  \u001b[38;5;66;03m# pragma: no cover\u001b[39;00m\n\u001b[0;32m    985\u001b[0m     \u001b[38;5;66;03m# caller's responsibility to check for this...\u001b[39;00m\n\u001b[1;32m--> 986\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\n\u001b[0;32m    987\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(columns)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m columns passed, passed data had \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    988\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(content)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m columns\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    989\u001b[0m     )\n\u001b[0;32m    990\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_mi_list:\n\u001b[0;32m    991\u001b[0m     \u001b[38;5;66;03m# check if nested list column, length of each sub-list should be equal\u001b[39;00m\n",
      "\u001b[1;31mAssertionError\u001b[0m: 10 columns passed, passed data had 1 columns",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[47], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDataFrame\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcrime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m df\u001b[38;5;241m.\u001b[39mto_excel(new_file_path,sheet_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBIOMETRICS\u001b[39m\u001b[38;5;124m\"\u001b[39m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\pandas\\core\\frame.py:851\u001b[0m, in \u001b[0;36mDataFrame.__init__\u001b[1;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[0;32m    849\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m columns \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    850\u001b[0m         columns \u001b[38;5;241m=\u001b[39m ensure_index(columns)\n\u001b[1;32m--> 851\u001b[0m     arrays, columns, index \u001b[38;5;241m=\u001b[39m \u001b[43mnested_data_to_arrays\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    852\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# error: Argument 3 to \"nested_data_to_arrays\" has incompatible\u001b[39;49;00m\n\u001b[0;32m    853\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# type \"Optional[Collection[Any]]\"; expected \"Optional[Index]\"\u001b[39;49;00m\n\u001b[0;32m    854\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    855\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    856\u001b[0m \u001b[43m        \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[0;32m    857\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    858\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    859\u001b[0m     mgr \u001b[38;5;241m=\u001b[39m arrays_to_mgr(\n\u001b[0;32m    860\u001b[0m         arrays,\n\u001b[0;32m    861\u001b[0m         columns,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    864\u001b[0m         typ\u001b[38;5;241m=\u001b[39mmanager,\n\u001b[0;32m    865\u001b[0m     )\n\u001b[0;32m    866\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\pandas\\core\\internals\\construction.py:520\u001b[0m, in \u001b[0;36mnested_data_to_arrays\u001b[1;34m(data, columns, index, dtype)\u001b[0m\n\u001b[0;32m    517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_named_tuple(data[\u001b[38;5;241m0\u001b[39m]) \u001b[38;5;129;01mand\u001b[39;00m columns \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    518\u001b[0m     columns \u001b[38;5;241m=\u001b[39m ensure_index(data[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39m_fields)\n\u001b[1;32m--> 520\u001b[0m arrays, columns \u001b[38;5;241m=\u001b[39m \u001b[43mto_arrays\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    521\u001b[0m columns \u001b[38;5;241m=\u001b[39m ensure_index(columns)\n\u001b[0;32m    523\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m index \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\pandas\\core\\internals\\construction.py:845\u001b[0m, in \u001b[0;36mto_arrays\u001b[1;34m(data, columns, dtype)\u001b[0m\n\u001b[0;32m    842\u001b[0m     data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mtuple\u001b[39m(x) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m data]\n\u001b[0;32m    843\u001b[0m     arr \u001b[38;5;241m=\u001b[39m _list_to_arrays(data)\n\u001b[1;32m--> 845\u001b[0m content, columns \u001b[38;5;241m=\u001b[39m \u001b[43m_finalize_columns_and_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    846\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m content, columns\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\pandas\\core\\internals\\construction.py:942\u001b[0m, in \u001b[0;36m_finalize_columns_and_data\u001b[1;34m(content, columns, dtype)\u001b[0m\n\u001b[0;32m    939\u001b[0m     columns \u001b[38;5;241m=\u001b[39m _validate_or_indexify_columns(contents, columns)\n\u001b[0;32m    940\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m    941\u001b[0m     \u001b[38;5;66;03m# GH#26429 do not raise user-facing AssertionError\u001b[39;00m\n\u001b[1;32m--> 942\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(err) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m    944\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(contents) \u001b[38;5;129;01mand\u001b[39;00m contents[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m np\u001b[38;5;241m.\u001b[39mobject_:\n\u001b[0;32m    945\u001b[0m     contents \u001b[38;5;241m=\u001b[39m convert_object_array(contents, dtype\u001b[38;5;241m=\u001b[39mdtype)\n",
      "\u001b[1;31mValueError\u001b[0m: 10 columns passed, passed data had 1 columns"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame(columns, columns=crime.columns)\n",
    "df.to_excel(new_file_path,sheet_name=\"BIOMETRICS\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
